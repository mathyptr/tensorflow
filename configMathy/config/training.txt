
!git clone https://github.com/tensorflow/models.git
%cd /content/models/research/
!protoc object_detection/protos/*.proto --python_out=.

# Install TensorFlow Object Detection API.
!cp object_detection/packages/tf2/setup.py .
!python -m pip install .

#test che fallisce
!python /content/models/research/object_detection/builders/model_builder_tf2_test.py

!pip install tensorflow --upgrade #da verificare se necessario. ## NON NECESSARIO
!pip install --upgrade tf-models-official  #da verificare se necessario ## NON NECESSARIO

!pip install tensorflow==2.13.1 #dopo questo chiede di il restart della session

#test che non fallisce con la 2.13.1
!python /content/models/research/object_detection/builders/model_builder_tf2_test.py


%cd /content/ #se non sei su colab crea una cartella content perchè gli script e comandi successivi lavorano a partire da tale cartella
!mkdir dataset
%cd dataset

!git clone https://github.com/mathyptr/tensorflow.git 
!mv tensorflow/kangaroodataset/* .


#se non posso scaricare da git e sto usando colab
#from google.colab import files
#uploaded = files.upload()
#!unzip kangaroodataset.zip #basta questo se sono solo in locale
#right-click in the File section on Google Colab and create a New file named labelmap.pbtxt as follows:
#item {
#    name: "kangaroo"
#    id: 1
#}


!mv tensorflow/labelmap.pbtxt /content
%cd /content/


!python dataset/generate_tf_records.py -l /content/labelmap.pbtxt -o dataset/train.record -i dataset/images -csv dataset/train_labels.csv
!python dataset/generate_tf_records.py -l /content/labelmap.pbtxt -o dataset/test.record -i dataset/images -csv dataset/test_labels.csv

%cd /content

!wget http://download.tensorflow.org/models/object_detection/classification/tf2/20200710/mobilenet_v2.tar.gz
!tar -xvf mobilenet_v2.tar.gz
!rm mobilenet_v2.tar.gz


!wget https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_mobilenet_v2_320x320_coco17_tpu-8.config
!mv ssd_mobilenet_v2_320x320_coco17_tpu-8.config mobilenet_v2.config




#Defining training parameters
num_classes = 1
batch_size = 96
num_steps = 7500
num_eval_steps = 1000

train_record_path = '/content/dataset/train.record'
test_record_path = '/content/dataset/test.record'
model_dir = '/content/training/'
labelmap_path = '/content/labelmap.pbtxt'

pipeline_config_path = 'mobilenet_v2.config'
fine_tune_checkpoint = '/content/mobilenet_v2/mobilenet_v2.ckpt-1'





import re

with open(pipeline_config_path) as f:
    config = f.read()

with open(pipeline_config_path, 'w') as f:

  # Set labelmap path
  config = re.sub('label_map_path: ".*?"', 
             'label_map_path: "{}"'.format(labelmap_path), config)
  
  # Set fine_tune_checkpoint path
  config = re.sub('fine_tune_checkpoint: ".*?"',
                  'fine_tune_checkpoint: "{}"'.format(fine_tune_checkpoint), config)
  
  # Set train tf-record file path
  config = re.sub('(input_path: ".*?)(PATH_TO_BE_CONFIGURED/train)(.*?")', 
                  'input_path: "{}"'.format(train_record_path), config)
  
  # Set test tf-record file path
  config = re.sub('(input_path: ".*?)(PATH_TO_BE_CONFIGURED/val)(.*?")', 
                  'input_path: "{}"'.format(test_record_path), config)
  
  # Set number of classes.
  config = re.sub('num_classes: [0-9]+',
                  'num_classes: {}'.format(num_classes), config)
  
  # Set batch size
  config = re.sub('batch_size: [0-9]+',
                  'batch_size: {}'.format(batch_size), config)
  
  # Set training steps
  config = re.sub('num_steps: [0-9]+',
                  'num_steps: {}'.format(num_steps), config)
  
  f.write(config)




#With the parameters set, start the training: 
!python /content/models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path={pipeline_config_path} \
    --model_dir={model_dir} \
    --alsologtostderr \
    --num_train_steps={num_steps} \
    --sample_1_of_n_eval_examples=1 \
    --num_eval_steps={num_eval_steps}


#Se compare ad un certo punto ^C vuole dire che è crashato, probabilmente ha usato tutta la RAM
# provare con le seguenti righe che portano al crash e poi colab dovrebbe dire che “Your session crashed.”
# e chiede se si vuole aumentare la RAM




#Validate the model
!python /content/models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path={pipeline_config_path} \
    --model_dir={model_dir} \
    --checkpoint_dir={model_dir} 

#Here we're going to run the code through a loop that waits for checkpoints to evaluate. Once the evaluation finishes, you're going to see the message:
#INFO:tensorflow:Waiting for new checkpoint at /content/training/
#Then you can stop the cell


#grafics
%load_ext tensorboard
%tensorboard --logdir '/content/training/'




#export from colab

with open('/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/utils/tf_utils.py') as f:
    tf_utils = f.read()

with open('/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/utils/tf_utils.py', 'w') as f:
  # Set labelmap path
  throw_statement = "raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))"
  tf_utils = tf_utils.replace(throw_statement, "if not isinstance(x, str):" + throw_statement)
  f.write(tf_utils)

output_directory = 'inference_graph'



!python /content/models/research/object_detection/exporter_main_v2.py \
    --trained_checkpoint_dir {model_dir} \
    --output_directory {output_directory} \
    --pipeline_config_path {pipeline_config_path}



#Testing the trained model

!wget https://raw.githubusercontent.com/hugozanini/object-detection/master/inferenceutils.py

from inferenceutils import *
output_directory = 'inference_graph/'
category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)
tf.keras.backend.clear_session()
model = tf.saved_model.load(f'/content/{output_directory}/saved_model')
import pandas as pd
test = pd.read_csv('/content/dataset/test_labels.csv')
#Getting 3 random images to test
images = list(test.sample(n=3)['filename'])

for image_name in images:
  image_np = load_image_into_numpy_array('/content/dataset/images/' + image_name)
  output_dict = run_inference_for_single_image(model, image_np)
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks_reframed', None),
      use_normalized_coordinates=True,
      line_thickness=8)
  display(Image.fromarray(image_np))




#comandi da usare se stai usando Colab
!zip -r /content/saved_model.zip /content/inference_graph/saved_model/
from google.colab import files
files.download("/content/saved_model.zip")



########################CONVERSIONE
virtualenv -p python3 venv
source venv/bin/activate
pip install tensorflowjs[wizard]

#questo comando da errore (sembra funzioni "spesso con python 3.6.8)
tensorflowjs_wizard

#se hai effettuato il training in locale per la conversione usa questo comando
tensorflowjs_converter     --input_format=tf_saved_model     --output_format=tfjs_graph_model     --signature_name=serving_default     --saved_model_tags=serve     /content/inference_graph/saved_model/     /content/mykangaroo/myweb_model

########################CONVERSIONE



########################UTILIZZO

#Per usare il modello creato dobbiamo creare un server HTTP che renderà il modello disponibile in un URL tramite richieste GET (il modello deve stare nella document root del server web).
Durante il caricamento del modello, TensorFlow.js eseguirà le seguenti richieste:
GET /model.json
GET /group1-shard1of5.bin
GET /group1-shard2of5.bin
GET /group1-shard3of5.bin
GET /group1-shardo4f5.bin
GET /group1-shardo5f5.bin

#scarico la web app di Zanini
git clone https://github.com/hugozanini/TFJS-object-detection

Nella web app va modificata nell'index.js la funzione load_model(), in modo da interrogare il web server che gestisce il nostro modello

#vi tensorflow/src/index.js
#modifica funzione
async function load_model() {
    //se il modello è in locale con webserver che ha come document root la directory contenente i file del modello (json e bin)
    //const model = await loadGraphModel("http://127.0.0.1:8080/model.json");
    
    //se uso docker e il modello è su un altro container su windows lo vede come localhost:8765
    const model = await loadGraphModel("http://localhost:8765/model.json");

    //se il webserver è su un'altra macchina
    //const model = await loadGraphModel("http://172.17.0.3:8080/model.json");

    //questo è il caso in cui prendiamo il modello dal git di Zanini
    //const model = await loadGraphModel("https://raw.githubusercontent.com/hugozanini/TFJS-object-detection/master/models/kangaroo-detector/model.json");
    return model;
  }

#se abbiamo a disposizione nodejs possiamo far partire la web app nel seguente modo
npm install
npm test
npm start build

#tramite browser ci possiamo collegare al servizio esposto da node js-> http://ipAddress_serverNodejs:portaSuCuiStaInAscoltoNodejs
########################UTILIZZO

